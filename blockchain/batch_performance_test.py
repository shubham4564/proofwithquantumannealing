#!/usr/bin/env python3
"""
Batch Performance Test Runner

Runs multiple performance tests with different configurations to analyze
blockchain performance under various loads and conditions.
"""

import sys
import os
import time
from datetime import datetime
import json

sys.path.insert(0, '/Users/shubham/Documents/proofwithquantumannealing/blockchain')

from flexible_performance_test import FlexiblePerformanceTest

class BatchTestRunner:
    """Run multiple performance tests with different configurations"""
    
    def __init__(self):
        self.test_results = []
    
    def run_test_scenario(self, name, num_transactions, batch_size, monitor_duration):
        """Run a single test scenario"""
        print(f"\nğŸ§ª SCENARIO: {name}")
        print(f"   ğŸ’³ Transactions: {num_transactions}")
        print(f"   ğŸ“¦ Batch Size: {batch_size}")
        print(f"   â±ï¸  Monitor: {monitor_duration}s")
        print("-" * 50)
        
        tester = FlexiblePerformanceTest()
        start_time = time.time()
        
        results = tester.run_performance_test(
            num_transactions=num_transactions,
            batch_size=batch_size,
            monitor_duration=monitor_duration
        )
        
        test_duration = time.time() - start_time
        
        if results:
            summary = results['summary']
            scenario_result = {
                'scenario_name': name,
                'config': {
                    'num_transactions': num_transactions,
                    'batch_size': batch_size,
                    'monitor_duration': monitor_duration
                },
                'test_duration_seconds': test_duration,
                'summary': summary,
                'key_metrics': {
                    'success_rate': summary['transaction_metrics']['success_rate_percent'],
                    'overall_tps': summary['throughput_metrics']['overall_tps'],
                    'blocks_created': summary['consensus_metrics']['total_blocks_created'],
                    'avg_transaction_time_ms': summary['transaction_metrics']['timing']['avg_total_transaction_time_ms'],
                    'avg_consensus_time_s': summary['consensus_metrics']['avg_consensus_time_seconds']
                }
            }
            
            self.test_results.append(scenario_result)
            
            print(f"\nâœ… SCENARIO COMPLETE: {name}")
            print(f"   ğŸ¯ Success Rate: {scenario_result['key_metrics']['success_rate']:.1f}%")
            print(f"   âš¡ TPS: {scenario_result['key_metrics']['overall_tps']:.2f}")
            print(f"   ğŸ“¦ Blocks: {scenario_result['key_metrics']['blocks_created']}")
            
            return scenario_result
        else:
            print(f"\nâŒ SCENARIO FAILED: {name}")
            return None
    
    def run_all_scenarios(self):
        """Run all predefined test scenarios"""
        print("ğŸš€ BATCH PERFORMANCE TEST RUNNER")
        print("ğŸ§ª Running multiple scenarios to analyze blockchain performance")
        print("=" * 70)
        
        scenarios = [
            # Small load tests
            ("Light Load", 25, 5, 15),
            ("Medium Load", 50, 10, 20),
            ("Heavy Load", 100, 20, 30),
            
            # Batch size comparison
            ("Small Batches", 60, 3, 20),
            ("Large Batches", 60, 30, 20),
            
            # Sustained load test
            ("Sustained Load", 150, 15, 45),
            
            # Stress test
            ("Stress Test", 200, 25, 60),
        ]
        
        for name, num_tx, batch_size, monitor_duration in scenarios:
            try:
                self.run_test_scenario(name, num_tx, batch_size, monitor_duration)
                
                # Cool-down period between tests
                print("   ğŸ˜´ Cooling down for 5 seconds...")
                time.sleep(5)
                
            except Exception as e:
                print(f"âŒ Scenario '{name}' failed with error: {e}")
                continue
    
    def generate_comparative_report(self):
        """Generate a comparative report across all test scenarios"""
        if not self.test_results:
            print("âŒ No test results to analyze")
            return
        
        print(f"\nğŸ“Š COMPARATIVE PERFORMANCE ANALYSIS")
        print("=" * 70)
        
        # Summary table
        print(f"\nğŸ“ˆ Scenario Comparison:")
        print(f"{'Scenario':<20} {'TPS':<8} {'Success%':<9} {'Blocks':<7} {'TxTime(ms)':<11} {'ConsTime(s)':<11}")
        print("-" * 70)
        
        for result in self.test_results:
            metrics = result['key_metrics']
            print(f"{result['scenario_name']:<20} "
                  f"{metrics['overall_tps']:<8.2f} "
                  f"{metrics['success_rate']:<9.1f} "
                  f"{metrics['blocks_created']:<7} "
                  f"{metrics['avg_transaction_time_ms']:<11.2f} "
                  f"{metrics['avg_consensus_time_s']:<11.3f}")
        
        # Best performance analysis
        print(f"\nğŸ† Best Performance Analysis:")
        
        best_tps = max(self.test_results, key=lambda x: x['key_metrics']['overall_tps'])
        best_success = max(self.test_results, key=lambda x: x['key_metrics']['success_rate'])
        best_blocks = max(self.test_results, key=lambda x: x['key_metrics']['blocks_created'])
        fastest_tx = min(self.test_results, key=lambda x: x['key_metrics']['avg_transaction_time_ms'])
        fastest_consensus = min(self.test_results, key=lambda x: x['key_metrics']['avg_consensus_time_s'])
        
        print(f"   âš¡ Highest TPS: {best_tps['scenario_name']} ({best_tps['key_metrics']['overall_tps']:.2f} TPS)")
        print(f"   âœ… Best Success Rate: {best_success['scenario_name']} ({best_success['key_metrics']['success_rate']:.1f}%)")
        print(f"   ğŸ“¦ Most Blocks: {best_blocks['scenario_name']} ({best_blocks['key_metrics']['blocks_created']} blocks)")
        print(f"   ğŸƒ Fastest Transactions: {fastest_tx['scenario_name']} ({fastest_tx['key_metrics']['avg_transaction_time_ms']:.2f}ms)")
        print(f"   âš¡ Fastest Consensus: {fastest_consensus['scenario_name']} ({fastest_consensus['key_metrics']['avg_consensus_time_s']:.3f}s)")
        
        # Performance recommendations
        print(f"\nğŸ’¡ Performance Recommendations:")
        
        avg_tps = sum(r['key_metrics']['overall_tps'] for r in self.test_results) / len(self.test_results)
        avg_success = sum(r['key_metrics']['success_rate'] for r in self.test_results) / len(self.test_results)
        
        if avg_tps >= 10:
            print(f"   âœ… Excellent throughput: Average {avg_tps:.2f} TPS across all scenarios")
        elif avg_tps >= 5:
            print(f"   ğŸŸ¡ Good throughput: Average {avg_tps:.2f} TPS - consider optimization for higher loads")
        else:
            print(f"   ğŸ”´ Limited throughput: Average {avg_tps:.2f} TPS - performance optimization needed")
        
        if avg_success >= 95:
            print(f"   âœ… Excellent reliability: {avg_success:.1f}% average success rate")
        elif avg_success >= 85:
            print(f"   ğŸŸ¡ Good reliability: {avg_success:.1f}% average success rate")
        else:
            print(f"   ğŸ”´ Reliability issues: {avg_success:.1f}% average success rate - investigate network issues")
        
        # Optimal configuration
        balanced_scores = []
        for result in self.test_results:
            metrics = result['key_metrics']
            # Balanced score considering TPS, success rate, and consistency
            score = (metrics['overall_tps'] * 0.4 + 
                    metrics['success_rate'] * 0.4 + 
                    (100 - metrics['avg_transaction_time_ms']) * 0.2)
            balanced_scores.append((score, result))
        
        best_balanced = max(balanced_scores, key=lambda x: x[0])[1]
        print(f"\nğŸ¯ Recommended Configuration:")
        print(f"   ğŸ“‹ Scenario: {best_balanced['scenario_name']}")
        print(f"   ğŸ’³ Transactions: {best_balanced['config']['num_transactions']}")
        print(f"   ğŸ“¦ Batch Size: {best_balanced['config']['batch_size']}")
        print(f"   âš¡ Performance: {best_balanced['key_metrics']['overall_tps']:.2f} TPS, {best_balanced['key_metrics']['success_rate']:.1f}% success")
    
    def save_results(self):
        """Save all test results to a file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"batch_performance_results_{timestamp}.json"
        
        report_data = {
            'timestamp': timestamp,
            'total_scenarios': len(self.test_results),
            'test_results': self.test_results
        }
        
        with open(filename, 'w') as f:
            json.dump(report_data, f, indent=2)
        
        print(f"\nğŸ’¾ Complete results saved to: {filename}")
        return filename

def main():
    """Main batch test execution"""
    print("ğŸ§ª BATCH PERFORMANCE TEST SUITE")
    print("ğŸ“Š Comprehensive blockchain performance analysis across multiple scenarios")
    print("=" * 80)
    
    runner = BatchTestRunner()
    
    # Run all scenarios
    runner.run_all_scenarios()
    
    # Generate comparative analysis
    runner.generate_comparative_report()
    
    # Save results
    runner.save_results()
    
    print(f"\nğŸ BATCH TESTING COMPLETE!")
    print(f"   ğŸ§ª Scenarios Run: {len(runner.test_results)}")
    print(f"   ğŸ“Š Full analysis and recommendations provided above")

if __name__ == "__main__":
    main()
